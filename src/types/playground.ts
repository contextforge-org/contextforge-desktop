// LLM Chat Playground Types
// Import types from autogenerated client for type safety
import type {
  ConnectInput,
  ChatInput,
  DisconnectInput,
  ServerInput,
  LlmInput,
  LlmProviderTypeEnum
} from '../lib/contextforge-client-ts';

// Re-export client types for convenience
export type { ConnectInput, ChatInput, DisconnectInput, ServerInput, LlmInput };

// Use the client's provider enum which supports more providers
export type LLMProvider = LlmProviderTypeEnum;

export type ServerSelectionMode = 'virtual' | 'custom';

export type SessionConfig = {
  userId: string;
  provider: LLMProvider;
  llmConfig: Record<string, unknown>;
  serverMode: ServerSelectionMode;
  virtualServerId?: string;
  mcpServer?: {
    url: string;
    transport: string;
    authToken?: string;
  };
  streaming: boolean;
  availableTools: string[];
};

export type ChatMessage = {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: string;
  toolsUsed?: string[];
  toolInvocations?: number;
  elapsedMs?: number;
  isStreaming?: boolean;
};

export type LLMConfig = {
  // Azure OpenAI
  azure_endpoint?: string;
  azure_deployment?: string;
  api_version?: string;
  
  // OpenAI / Anthropic
  api_key?: string;
  model?: string;
  
  // Common
  temperature?: number;
  max_tokens?: number;
  top_p?: number;
  
  // Provider-specific
  [key: string]: unknown;
};

export type ConnectionStatus = {
  connected: boolean;
  provider?: string;
  toolCount?: number;
  tools?: string[];
  error?: string;
};

export type ConnectResponse = {
  status: string;
  user_id: string;
  provider: string;
  tool_count: number;
  tools: string[];
};

export type ChatResponse = {
  user_id: string;
  response: string;
  tool_used: boolean;
  tools: string[];
  tool_invocations: number;
  elapsed_ms: number;
};

export type DisconnectResponse = {
  status: 'disconnected' | 'no_active_session' | 'disconnected_with_errors';
  user_id: string;
  message: string;
  warning?: string;
};

export type StatusResponse = {
  user_id: string;
  connected: boolean;
};

export type ConfigResponse = {
  mcp_server: {
    url: string;
    transport: string;
  };
  llm: {
    provider: string;
    config: Record<string, unknown>;
  };
  enable_streaming: boolean;
};

// Provider configuration templates
// Supports all providers from LlmProviderTypeEnum:
// 'openai' | 'azure_openai' | 'anthropic' | 'bedrock' | 'google_vertex' | 'watsonx' |
// 'ollama' | 'openai_compatible' | 'cohere' | 'mistral' | 'groq' | 'together'
export const PROVIDER_CONFIGS: Record<LLMProvider, Partial<LLMConfig>> = {
  openai: {
    api_key: '',
    model: 'gpt-4',
    temperature: 0.7,
    max_tokens: 2000
  },
  azure_openai: {
    azure_endpoint: '',
    azure_deployment: 'gpt-4',
    api_version: '2024-02-15-preview',
    temperature: 0.7,
    max_tokens: 2000
  },
  anthropic: {
    api_key: '',
    model: 'claude-3-5-sonnet-20241022',
    temperature: 0.7,
    max_tokens: 4000
  },
  bedrock: {
    model: 'anthropic.claude-3-sonnet-20240229-v1:0',
    temperature: 0.7,
    max_tokens: 2000
  },
  google_vertex: {
    model: 'gemini-pro',
    temperature: 0.7,
    max_tokens: 2000
  },
  watsonx: {
    model: 'ibm/granite-13b-chat-v2',
    temperature: 0.7,
    max_tokens: 2000
  },
  ollama: {
    model: 'llama3',
    temperature: 0.7,
    base_url: 'http://localhost:11434'
  },
  openai_compatible: {
    api_key: '',
    model: '',
    base_url: '',
    temperature: 0.7,
    max_tokens: 2000
  },
  cohere: {
    api_key: '',
    model: 'command-r-plus',
    temperature: 0.7,
    max_tokens: 2000
  },
  mistral: {
    api_key: '',
    model: 'mistral-large-latest',
    temperature: 0.7,
    max_tokens: 2000
  },
  groq: {
    api_key: '',
    model: 'llama-3.1-70b-versatile',
    temperature: 0.7,
    max_tokens: 2000
  },
  together: {
    api_key: '',
    model: 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',
    temperature: 0.7,
    max_tokens: 2000
  }
};

// Provider display names
export const PROVIDER_NAMES: Record<LLMProvider, string> = {
  openai: 'OpenAI',
  azure_openai: 'Azure OpenAI',
  anthropic: 'Anthropic (Claude)',
  bedrock: 'AWS Bedrock',
  google_vertex: 'Google Vertex AI',
  watsonx: 'IBM Watsonx',
  ollama: 'Ollama (Local)',
  openai_compatible: 'OpenAI Compatible',
  cohere: 'Cohere',
  mistral: 'Mistral AI',
  groq: 'Groq',
  together: 'Together AI'
};

// Common models for each provider
export const PROVIDER_MODELS: Record<LLMProvider, string[]> = {
  openai: ['gpt-4', 'gpt-4-turbo', 'gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo'],
  azure_openai: ['gpt-4', 'gpt-4-turbo', 'gpt-4o', 'gpt-4o-mini', 'gpt-35-turbo'],
  anthropic: [
    'claude-3-5-sonnet-20241022',
    'claude-3-5-haiku-20241022',
    'claude-3-opus-20240229',
    'claude-3-sonnet-20240229',
    'claude-3-haiku-20240307'
  ],
  bedrock: [
    'anthropic.claude-3-sonnet-20240229-v1:0',
    'anthropic.claude-3-haiku-20240307-v1:0',
    'anthropic.claude-v2:1',
    'meta.llama3-70b-instruct-v1:0'
  ],
  google_vertex: [
    'gemini-pro',
    'gemini-pro-vision',
    'gemini-1.5-pro',
    'gemini-1.5-flash'
  ],
  watsonx: [
    'ibm/granite-13b-chat-v2',
    'ibm/granite-20b-multilingual',
    'meta-llama/llama-3-70b-instruct'
  ],
  ollama: ['llama3', 'llama2', 'mistral', 'mixtral', 'codellama', 'phi', 'qwen'],
  openai_compatible: [],
  cohere: [
    'command-r-plus',
    'command-r',
    'command',
    'command-light'
  ],
  mistral: [
    'mistral-large-latest',
    'mistral-medium-latest',
    'mistral-small-latest',
    'open-mistral-7b',
    'open-mixtral-8x7b'
  ],
  groq: [
    'llama-3.1-70b-versatile',
    'llama-3.1-8b-instant',
    'mixtral-8x7b-32768',
    'gemma-7b-it'
  ],
  together: [
    'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',
    'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',
    'mistralai/Mixtral-8x7B-Instruct-v0.1',
    'Qwen/Qwen2-72B-Instruct'
  ]
};

// Made with Bob
